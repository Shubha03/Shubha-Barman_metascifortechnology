{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Algorithm questions**"
      ],
      "metadata": {
        "id": "s0Tp8v1kP9jY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.How does regularization (L1 and L2) help in preventing overfitting?\n",
        "\n",
        "Ans:\n",
        "Regularization techniques like L1 and L2 help prevent overfitting by adding a penalty term to the model's loss function that discourages overly complex models. Here's a detailed breakdown:\n",
        "\n",
        "\n",
        "\n",
        "L2 Regularization (Ridge Regression):\n",
        "Adds squared magnitude of coefficients to the loss function\n",
        "Penalizes large weight values\n",
        "Encourages weights to be small and distributed evenly\n",
        "Helps reduce model complexity\n",
        "Effective for continuous features\n",
        "Mathematically: Loss = Original Loss + λ * (sum of squared weights)\n",
        "\n",
        "\n",
        "\n",
        "L1 Regularization (Lasso Regression):\n",
        "Adds absolute value of coefficients to the loss function\n",
        "Encourages sparsity in the model\n",
        "Can drive some weights to exactly zero\n",
        "Performs feature selection by eliminating less important features\n",
        "Mathematically: Loss = Original Loss + λ * (sum of absolute weights)"
      ],
      "metadata": {
        "id": "ceuM1qiXNyew"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Why is feature scaling important in gradient descent?\n",
        "\n",
        "Ans:\n",
        "Feature scaling is crucial for efficient and effective gradient descent because it ensures that all features contribute equally to the optimization process. Without scaling, features with larger values will dominate the gradient updates, leading to slow convergence and potentially preventing the algorithm from finding a good solution. Scaling helps create a more balanced and symmetrical optimization landscape, resulting in faster convergence and improved model performance."
      ],
      "metadata": {
        "id": "37TZQZBdOH_e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem Solving**"
      ],
      "metadata": {
        "id": "s9_9Bbc8QKkq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Given a dataset with missing values, how would you handle them before training an ML model?\n",
        "\n",
        "Ans:\n",
        "Handling missing values depends heavily on the dataset's characteristics and the chosen ML model. Here's a breakdown of common strategies:\n",
        "\n",
        "1. Deletion\n",
        "2. Imputation\n",
        "\n",
        "Advanced Techniques:\n",
        "\n",
        "1. Expectation-Maximization (EM) Algorithm\n",
        "2. Generative Models (e.g., GANs)"
      ],
      "metadata": {
        "id": "pWSvw0pSQXPE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "2.Design a pipeline for building a classification model. Include steps for data preprocessing.\n",
        "\n",
        "Ans:\n",
        "\n"
      ],
      "metadata": {
        "id": "FyOZ2CX4RfIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "# ... (load data, define features, and target variable) ...\n",
        "\n",
        "# Create a preprocessing pipeline\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_features),\n",
        "        ('cat', OneHotEncoder(), categorical_features)\n",
        "    ])\n",
        "\n",
        "# Create the complete pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='mean')),  # Impute missing values\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', RandomForestClassifier())\n",
        "])\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Train the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = pipeline.score(X_test, y_test)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "roALmJwpR_56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a comprehensive pipeline for building a classification model, including data preprocessing steps:\n",
        "\n",
        "1. Data Loading and Exploration\n",
        "2. Data Preprocessing\n",
        "3. Model Selection and Training\n",
        "4. Model Evaluation\n",
        "5. Model Deployment and Monitoring"
      ],
      "metadata": {
        "id": "4igaRS1nSW2c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Coding**"
      ],
      "metadata": {
        "id": "TcyyvNRpTBkB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Write a Python script to implement a decision tree classifier using Scikit-learn.\n",
        "\n",
        "Ans:\n"
      ],
      "metadata": {
        "id": "odG2OvxWTNmI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "data = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
        "data['target'] = iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X = data.drop('target', axis=1)\n",
        "y = data['target']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize and train the Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42) #You can adjust parameters here\n",
        "clf = clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate the model (example: accuracy)\n",
        "accuracy = clf.score(X_test, y_test)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# Visualize the decision tree (optional)\n",
        "plt.figure(figsize=(15, 10))\n",
        "plot_tree(clf, filled=True, feature_names=iris.feature_names, class_names=iris.target_names)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ptS4VGMMThq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Given a dataset, write code to split the data into training and testing sets using an 80-20 split.\n",
        "\n",
        "\n",
        "Ans:\n"
      ],
      "metadata": {
        "id": "0Swn0MZYTjtw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Sample dataset (replace with your actual data)\n",
        "data = {'column1': [1, 2, 3, 4, 5, 6, 7, 8],\n",
        "        'column2': [10, 20, 30, 40, 50, 60, 70, 80]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Split the data into training and testing sets (80-20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(df[['column1']], df['column2'], test_size=0.2)\n",
        "\n",
        "print(\"Training set:\")\n",
        "print(X_train)\n",
        "print(y_train)\n",
        "\n",
        "print(\"\\nTesting set:\")\n",
        "print(X_test)\n",
        "print(y_test)"
      ],
      "metadata": {
        "id": "8lQLsBBMTtFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Case Study**"
      ],
      "metadata": {
        "id": "akJ0E_r7UKL9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A company wants to predict employee attrition. What kind of ML problem is this? Which algorithms would you choose and why?\n",
        "\n",
        "Ans:\n",
        "\n",
        "Problem Type:\n",
        "Predicting employee attrition is a binary classification problem in machine learning. We are trying to classify employees into two categories: those who will leave the company (attrition = 1) and those who will stay (attrition = 0).\n",
        "\n",
        "\n",
        "\n",
        "Algorithm Choices:\n",
        "Several algorithms are well-suited for this task. Here are some popular choices:\n",
        "\n",
        " * Logistic Regression:\n",
        "   * Why: Simple, interpretable, and efficient for binary classification problems.\n",
        "   * How: It models the probability of an employee leaving based on various factors like job satisfaction, work-life balance, salary, etc.\n",
        "\n",
        " * Decision Trees:\n",
        "   * Why: Easy to understand, can handle both numerical and categorical data, and can capture complex interactions between features.\n",
        "   * How: They create a tree-like model of decisions and their possible consequences.\n",
        "\n",
        " * Random Forest:\n",
        "   * Why: Ensembles multiple decision trees to improve accuracy and reduce overfitting.\n",
        "   * How: It creates multiple decision trees and averages their predictions.\n",
        "\n",
        "\n",
        "Choosing the Best Algorithm:\n",
        "\n",
        "\n",
        "The best algorithm for your specific problem will depend on several factors:\n",
        "\n",
        " * Data Quality and Quantity: If your data is clean and has a sufficient number of samples, simpler models like logistic regression or decision trees might be enough. For larger and more complex datasets, ensemble methods like random forest.\n",
        "\n",
        " * Interpretability: If understanding the decision-making process is important, decision trees and logistic regression are more interpretable than complex models.\n",
        "\n",
        " * Model Performance: Experiment with different algorithms and evaluate their performance using metrics like accuracy, precision, recall.\n",
        " * Computational Resources: Consider the computational cost of training and predicting with different algorithms, especially for large datasets.\n"
      ],
      "metadata": {
        "id": "YL0N2b5qUUgt"
      }
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}